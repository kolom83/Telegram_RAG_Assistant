{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyP4w0QF++6dl+wMEKpcC/x5"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install -r requirements.txt"
      ],
      "metadata": {
        "id": "avEEs29NADFu",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# pip install python-docx aiogram langchain-community langchain-openai faiss-cpu"
      ],
      "metadata": {
        "collapsed": true,
        "id": "ou7KNUAHB8aZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile /content/bot.py\n",
        "import asyncio\n",
        "from aiogram import Bot, Dispatcher, types, F\n",
        "from aiogram.filters import Command\n",
        "from aiogram.enums import ParseMode\n",
        "from aiogram.client.default import DefaultBotProperties\n",
        "\n",
        "from langchain.schema import StrOutputParser\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.runnables import RunnablePassthrough\n",
        "from docx import Document\n",
        "\n",
        "from langchain.text_splitter import (\n",
        "    CharacterTextSplitter,\n",
        "    RecursiveCharacterTextSplitter,\n",
        ")\n",
        "\n",
        "# from langchain.embeddings import HuggingFaceEmbeddings\n",
        "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
        "# from langchain.vectorstores import FAISS\n",
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "# Инициализация RAG\n",
        "def initialize_rag():\n",
        "  # загружаем документ\n",
        "  doc = Document(\"/content/доработки Диадок.docx\")\n",
        "  text = \"\\n\".join([paragraph.text for paragraph in doc.paragraphs])\n",
        "\n",
        "  # Определяем сплиттер, делим текст на кусочки\n",
        "  splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=500,\n",
        "    chunk_overlap=100,\n",
        "    length_function=len,\n",
        "  )\n",
        "  split_documents = splitter.create_documents([text])\n",
        "\n",
        "  # Если у вас нет видеокарты, укажите 'device': 'cpu'\n",
        "  # Задаём embedding model\n",
        "  hf_embeddings_model = HuggingFaceEmbeddings(\n",
        "    model_name=\"cointegrated/LaBSE-en-ru\", model_kwargs={\"device\": \"cuda\"}\n",
        "  )\n",
        "\n",
        "  # Создаём векторное хранилище\n",
        "  db = FAISS.from_documents(\n",
        "        split_documents, hf_embeddings_model\n",
        "  )\n",
        "\n",
        "  db.save_local(\"faiss_db\")\n",
        "\n",
        "  # Задаём ретривер\n",
        "  # Ретривер - это такая надстройка над базой данных, которая может выдавать похожие документы, но не обязана их хранить.\n",
        "  retriever = db.as_retriever()\n",
        "\n",
        "  # Создаём простой шаблон\n",
        "  template = \"\"\"\n",
        "    Отвечайте на вопрос, основываясь только на следующем контексте:\n",
        "\n",
        "    {context}\n",
        "\n",
        "    Question: {question}\n",
        "  \"\"\"\n",
        "  # Создаём промпт из шаблона\n",
        "  prompt = ChatPromptTemplate.from_template(template)\n",
        "\n",
        "  # Укажите base_url и api_key для OpenRouter\n",
        "  llm = ChatOpenAI(\n",
        "    temperature=0.7,\n",
        "    openai_api_key=OPENAI_API_KEY,  # Ваш API-ключ OpenRouter\n",
        "    base_url=\"https://openrouter.ai/api/v1\",\n",
        "    model=\"openai/gpt-4o\"\n",
        "    # model=\"mistralai/mistral-7b-instruct\"  # Укажите нужную модель\n",
        "    , max_tokens=1000\n",
        "    )\n",
        "\n",
        "\n",
        "  # Объявляем функцию, которая будет собирать строку из полученных документов\n",
        "  def format_docs(docs):\n",
        "      return \"\\n\\n\".join([d.page_content for d in docs])\n",
        "\n",
        "\n",
        "  # Создаём цепочку\n",
        "  chain = (\n",
        "        {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
        "        | prompt\n",
        "        | llm\n",
        "        | StrOutputParser()\n",
        "        )\n",
        "\n",
        "  return chain\n",
        "\n",
        "# В ячейке Colab (лучше для безопасности)\n",
        "TELEGRAM_BOT_TOKEN = input(\"Введите TELEGRAM_BOT_TOKEN: \")\n",
        "OPENAI_API_KEY = input(\"Введите OPENAI_API_KEY: \")\n",
        "\n",
        "# Инициализируем цепочку RAG при старте\n",
        "chain = initialize_rag()\n",
        "\n",
        "dp = Dispatcher()\n",
        "\n",
        "@dp.message(Command('start'))\n",
        "async def start_command(message: types.Message) -> None:\n",
        "     await message.answer(f'<b>{message.from_user.full_name}</b>, добро пожаловать в бот помошник по данным, задай вопрос по теме!')\n",
        "\n",
        "@dp.message()\n",
        "async def echo_handler(message: types.Message) -> None:\n",
        "    try:\n",
        "        # Получаем ответ от цепочки RAG\n",
        "        response = await chain.ainvoke(message.text)\n",
        "        await message.reply(response)\n",
        "    except Exception as e:\n",
        "        await message.reply(f\"Произошла ошибка: {str(e)}\")\n",
        "\n",
        "\n",
        "async def main() -> None:\n",
        "    token = TELEGRAM_BOT_TOKEN\n",
        "    bot = Bot(token\n",
        "              ,default = DefaultBotProperties(parse_mode = ParseMode.HTML)\n",
        "              )\n",
        "    await dp.start_polling(bot)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    asyncio.run(main())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y8yrh6jwyxTX",
        "outputId": "dd89a788-b47a-4556-b9ed-1f13c25af9d9"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing /content/bot.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! python bot.py"
      ],
      "metadata": {
        "collapsed": true,
        "id": "EzzmEllVzCd1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "x2zX2Fh8Nfmc"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}